{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Bayesian Updates\n",
    "\n",
    "Sequential Bayesian updates are a key model for learning with uncertainty in cognitive science.  The decision maker/agent updates their understanding of a latent state $s$ in a context $c$ given an additional observation $y_{T}$ in a history of observations $\\mathscr{H}_T = \\{y_0,\\cdots,y_{T-1} \\}$ according to the recursive formula:\n",
    "$$ p_T(s|y_{T},c,\\mathscr{H}_T) = \\frac{p(y_T|s,c) p_{T-1}(s|c,\\mathscr{H}_T)}{\\int_s p(y_T|s,c) p_{T-1}(s|c,\\mathscr{H}_T) ds}$$\n",
    "with the initial condition $p_0(s|c,\\mathscr{H}_0) = p(s|c)$, which represents the typical distribution of states of $s$ in context $c$.\n",
    "\n",
    "Adding a transition kernel $p(s',c',\\mathscr{H}_T'|s,c,\\mathscr{H}_T,a)$ to this equation produces an important generalization of updating to cases where the state, context or validity of the history can change between updates. Transition kernels can model effects like context shifting, action dependent beliefs, forgetting, world dynamics and more.  The inclusion of the history is unusual (we aren't aware of previous treatments), but we will show that it is needed to model a core problem in Bayesian updating that occurs when data is not guaranteed to be valid.\n",
    "\n",
    "In fact, a core problem with Bayesian updates is that they assume the agent (decision-maker) conditions on data that is valid and relevant. However, there are several common cases that violate this condition, including important cases that have not had adequate solutions. First is where there is valid data available but there are difficulties observing it.  Second is where there is data available from many contexts and the validity of data for the current context depends on its provenance (relevance/compatibility of contexts). Third is where a mixture of valid and invalid data are available that are not easily distinguished.  In this development we focus on the first two problems whose solution useful for the more difficult full case.\n",
    "\n",
    "The first case, where observations (data) are missing (and the agent is aware of it) has a well-known solution. Bayes rule in this case involves marginalizing over the missing data. Less clear is how updating may work if the agent is not certain that observations are missing.  A key example of the second case is when some data are generated (counter-factually), representing *possible data* or data histories.  Counter-factual data are used extensively in sampling based inference strategies for a range of probabilistic computations, (e.g. marginalize latent variables, compute likelihoods for a prospective outcomes given actions, or compute expectations of key decision variables). \n",
    "\n",
    "However, counter-factual samples provide no actual evidence for a claim, therefore it is essential to represent clearly the distinction between an internally sampled counter-factual value and a valid observation - the provenance of the sample must be maintained.  Note that the key distinction is the sample's validity as evidence. By evidence we mean a set of samples that serve as $witnesses$ by either being valid observations or represent knowledge assumed certain.  The validity of an observation hinges on whether it is compatible with the decision context (from an equivalent context, prediction relevant, recent enough, etc).  It is common for generated samples to themselves be prior observations.  because concrete observations may underlie the probability functions (e.g. kernal methods), may stem from different contexts, or might be previous observations labeled as invalid for conditioning.  In any of these cases, the internal samples are *memories* with meta-data that is essential to determine their status.  \n",
    "\n",
    "Here we develop a theory of probabilistic updating that includes meta-data attached to memory samples to allow updates to respect validation and relevance operations. The core problem is to understand how different types of data \n",
    "\n",
    "In probability theory it is common to use upper case and lower case to distinguish when a variable represents a free variable vs. an instantiated value.  There is not as common a notation to distinguish observed values from putative (sampled) values.  Here we will use lower case variables with superscripts to indicate a putative value of a variable, and subscripts to indicate an observed value.  \n",
    "\n",
    "\n",
    " Second, any marginalizations are implemented by approximating the integrals by summing over a set of samples from the prior or likelihood.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
